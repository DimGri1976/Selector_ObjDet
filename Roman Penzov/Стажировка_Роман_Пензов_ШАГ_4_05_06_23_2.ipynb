{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Untick/Selector_ObjDet/blob/Roman-Penzov-folder/%D0%A1%D1%82%D0%B0%D0%B6%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0_%D0%A0%D0%BE%D0%BC%D0%B0%D0%BD_%D0%9F%D0%B5%D0%BD%D0%B7%D0%BE%D0%B2_%D0%A8%D0%90%D0%93_4_05_06_23_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80eKfaOzhsyl"
      },
      "source": [
        "# **ШАГ 4.**\n",
        "Мысли нарастающим итогом:\n",
        "1.   Как и на шаге 1 для первоначального решения задачи не стал вычищать базу, а оставил только файлы формата jpg и jpeg. Некоторые папки (виды одежды) удалил целиком. А уже на заключительных шагах буду использовать \"исправленную/актуализированную\" базу.\n",
        "2.   Также хочу пробовать следующий алгоритм:\n",
        "  *   сначала первая нейронка должна определить категорию одежды, к которой относится\"картинка клиента\". И почему бы для этого не использовать для первого варианта обычные сверточные нейронные сети. Иначе возможно попробовать вариант проверки на выброс (ШАГ_4).\n",
        "  *   также отдельно сделать \"эталонную\" политру по цветам, на основании которой сортировать изображения\n",
        "  *   второй вид нейронки - это вариационный автокодировщик, который надо реализовать для каждой категории отдельно. Почему то думается, что так проще и эффективнее.\n",
        "3. Написать функцию подбора схожих изображений на основе encoder.predict, NearestNeighbors (ШАГ_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K_F0-D-3Nega"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TT7wxvcQj86y"
      },
      "outputs": [],
      "source": [
        "# Работа с массивами\n",
        "\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "# Основа для создания последовательной модели\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Основные слои\n",
        "from tensorflow.keras.layers import MaxPooling2D, Dropout, BatchNormalization\n",
        "\n",
        "# Оптимизатор\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad\n",
        "\n",
        "# Матрица ошибок классификатора\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Подключение модуля для загрузки данных из облака\n",
        "import gdown\n",
        "\n",
        "# Для работы с файлами\n",
        "import gc\n",
        "\n",
        "# Рисование графиков в ячейках Colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBFdFmT1kco2",
        "outputId": "6bb11373-a655-4e99-d2b0-dd800beb043b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Подключение к Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VfvS2dfPkcmH"
      },
      "outputs": [],
      "source": [
        "# Путь к архивному файлу\n",
        "file_path = '/content/gdrive/MyDrive/test_230501.zip'\n",
        "\n",
        "# Путь к директории, в которую нужно разархивировать файл\n",
        "extract_path = '/content/selector'\n",
        "\n",
        "# Разархивирование файла\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qh4KwL21kb6X"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/selector/test_230501/'                  # Путь к базе\n",
        "os.listdir(data_dir)\n",
        "batch_size= 32\n",
        "image_size= (256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru4DdfTBmfko",
        "outputId": "064b6375-311a-4379-92a0-f8732b07334a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7650 files belonging to 9 classes.\n",
            "Using 6120 files for training.\n",
            "Found 7650 files belonging to 9 classes.\n",
            "Using 1530 files for validation.\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,    # Устанавливаем разделение на обучающую и проверочную выборки\n",
        "  subset=\"training\",       # Эта выборка обучающая\n",
        "  seed=123,                # закрепляем рандом для сравнения сетей\n",
        "  image_size= image_size,\n",
        "  batch_size= batch_size)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",     # Эта выборка проверочная\n",
        "  seed=123,\n",
        "  image_size= image_size,\n",
        "  batch_size= batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoL6WIzRmfhd",
        "outputId": "006d5126-f0d6-4b46-b1b5-320a45ca7404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dress  business', 'dress casual', 'dress homemade', 'dress solemn', 'shirt men', 'shirt women', 'sportswear women', 't-shirt men', 't-shirt women']\n"
          ]
        }
      ],
      "source": [
        "class_names = train_ds.class_names # Определяем имена классов\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nIjL9lQNmfej"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE    # предварительная подгрузка датасета для ускорения и стабилизации\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE) \n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jpdeb06ZoBD8"
      },
      "outputs": [],
      "source": [
        "# Аугментация в виде слоя для модели\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomZoom((-0.05,0.05)), # произвольное увеличение и уменьшение на 10%\n",
        "  tf.keras.layers.experimental.preprocessing.RandomContrast(0.15)])    # изменение контраста изображений "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "stVUrg0KoBBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab5aca4-8034-4401-8674-149dc44bcea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Слой подготовки данных для vgg16(у каждой модели свой)\n",
        "preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
        "gc.collect()\n",
        "\n",
        "# Создание модели\n",
        "image_shape = image_size + (3,)\n",
        "base_model = tf.keras.applications.vgg16.VGG16(input_shape=image_shape,\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "\n",
        "# base_model.summary()\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D() # Глобал пулинг\n",
        "prediction_layer = tf.keras.layers.Dense(len(class_names), activation='softmax') # По количеству классов = 3 класса\n",
        "\n",
        "# собираем полную модель \n",
        "inputs = tf.keras.Input(shape=(image_shape))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fHE8OmXSoA-v"
      },
      "outputs": [],
      "source": [
        "# компилируем\n",
        "\n",
        "base_learning_rate = 0.00001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate= base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "initial_epochs = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "JouVGxhzodWZ",
        "outputId": "662e3734-0203-4d25-8491-67e91c9ba9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            " 64/192 [=========>....................] - ETA: 1:10 - loss: 2.4573 - accuracy: 0.2920"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d12b4a472da4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# обучаем\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m hVGG16 = model.fit(train_ds,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=val_ds)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_3979]"
          ]
        }
      ],
      "source": [
        "# обучаем\n",
        "\n",
        "hVGG16 = model.fit(train_ds,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=val_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbhJSHLm4wn26jpi0/d61O",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}